{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Advanced Training (from scratch) - Part 2\n",
    "\n",
    "In this notebook, you will adapt your code from Jupyter Notebook 3 to train a feedforward neural network (FNN) on two different datasets: \n",
    "\n",
    "- Lincoln Home Sales: contains data of single family homes sold between January 2016 and August 2022 with the features\n",
    "    - Overall Rating of Home Condition\n",
    "    - Rating of Building Material Quality\n",
    "    - Year Remodeled\n",
    "    - Remodel Type\n",
    "    - Total Living Area\n",
    "    - Year Built\n",
    "    - Garage Capacity\n",
    "    - Bedroom Count\n",
    "    - Total Basement Area\n",
    "    - Minimally Finished Basement Area\n",
    "    - Completely Finished Basement Area\n",
    "    - Fireplace Count\n",
    "    - Number of Fixtures\n",
    "    - Pool Area\n",
    "    - Total Acres\n",
    "    - Sale Date\n",
    "    - Price\n",
    "\n",
    "- MNIST: contains handwritten digits 0-9\n",
    "\n",
    "In part 1, you trained on the Lincoln Home Sales. Now we turn our attention to the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "To train the MNIST dataset effectively, we need to explore the following adjustments to your training functions:\n",
    "\n",
    "1. using a new loss function called *Categorical Crossentropy*,\n",
    "\n",
    "1. implementing a different activation function in the final layer than in the hidden layer, and\n",
    "\n",
    "1. building a FNN with multiple hidden layers of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll load the MNIST dataset for you (this is the only code cell that your allowed to use TensorFlow)\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy\n",
    "\n",
    "Recall when we first constructed a model for the MNIST dataset in the FNN Slides and Project 1, we utilized the softmax activation function\n",
    "    $$ \\text{softmax}({\\bf z}) = \\frac{1}{\\sum_{j=1}^k e^{z_j}} \\begin{bmatrix} e^{z_1} \\\\ e^{z_2} \\\\ \\vdots \\\\ e^{z_k} \\end{bmatrix} .$$\n",
    "Remember, this activation function yields a probability vector.\n",
    "\n",
    "\n",
    "\n",
    "**Definition:** Consider the label of a single sample \n",
    "    $$ {\\bf y} = \\begin{bmatrix} y_1\\\\ y_2 \\\\ \\vdots \\\\ y_k \\end{bmatrix} $$\n",
    "Then the *Categorical Cross-Entropy* loss function is defined as follows: \n",
    "    $$ CE({\\bf y}, \\tilde{\\bf y}) = -\\sum_{j=1}^k y_j \\ln(\\tilde{y}_j) $$\n",
    "\n",
    "\n",
    "\n",
    "Categorical Cross-Entropy is ideal for classification neural networks. Specifically, it is the usual loss function used when softmax is applied in the final layer of a nueral network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 1**\n",
    "\n",
    "Define and compute functions for Categorical Cross-Entropy and $\\nabla_{{\\bf a}^F} CE$. Recall that\n",
    "\n",
    "$$ \\nabla_{{\\bf a}^F} CE = \\begin{bmatrix} \\dfrac{\\partial CE}{\\partial a^F_1} \\\\ ~ \\\\ \\dfrac{\\partial CE}{\\partial a^F_2} \\\\ ~\\\\ \\vdots \\\\ ~ \\\\ \\dfrac{\\partial CE}{\\partial a^F_k} \\end{bmatrix} , $$\n",
    "\n",
    "where $k$ is the number of neurons in the final layer and \n",
    "\n",
    "$$ CE({\\bf y}, \\tilde{\\bf y}) = CE({\\bf y}, {\\bf a}^F) = -\\sum_{j=1}^k y_j \\ln({a}^F_j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalCrossEntropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Add your code below\n",
    "    \"\"\"\n",
    "    CE = \n",
    "    return CE\n",
    "\n",
    "# test your function with some example values\n",
    "y_true = np.array([1, 0, 0, 0])\n",
    "y_pred = np.array([0.7, 0.1, 0.1, 0.1])\n",
    "print(CategoricalCrossEntropy(y_true, y_pred)) # should be 0.35667494393873245\n",
    "\n",
    "def grad_CategoricalCrossEntropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Add your code below\n",
    "    \"\"\"\n",
    "    grad = \n",
    "    return grad\n",
    "\n",
    "# test your function with some example values\n",
    "print(grad_CategoricalCrossEntropy(y_true, y_pred)) # should be array([-1.42857143,  0.        ,  0.        ,  0.        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2A**\n",
    "\n",
    "Define a function for softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "        Add your code below\n",
    "    \"\"\"\n",
    "    s =\n",
    "    return s\n",
    "\n",
    "# test your function with some example values\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "print(softmax(x)) # should be array([0.65900114, 0.24243297, 0.09856589])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2B**\n",
    "\n",
    "Note that softmax is vector-valued function. So, its \"derivative\" is more complicated than functions like sigmoid or ReLU. In fact, we must compute $\\dfrac{\\partial a^F_k}{\\partial z^F_j}$ for all $1\\leq j, k \\leq n_F$ where $n_F$ is the number of neurons in the final layer. So,\n",
    "$$  {\\bf a}^F = \\text{softmax}({\\bf z^F}) = \\frac{1}{\\sum_{i=1}^{n_F} e^{z^F_i}} \\begin{bmatrix} e^{z^F_1} \\\\ e^{z^F_2} \\\\ \\vdots \\\\ e^{z^F_{n_F}} \\end{bmatrix} , $$\n",
    "and\n",
    "$$ a^F_k = \\dfrac{e^{z^F_k}}{\\sum_{i=1}^{n_F} e^{z^F_i}} .$$\n",
    "\n",
    "Compute $\\dfrac{\\partial a^F_k}{\\partial z^F_j}$. *Hint: your answer should be a peicewise function based on $j$ and $k$.*\n",
    "\n",
    "For this problem, you do not need to program anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2C**\n",
    "\n",
    "Notice that $\\dfrac{\\partial a^F_k}{\\partial z^F_j}$ is dependent on two indices. So, we use a matrix to record all of these partials:\n",
    "\n",
    "$$ \\nabla_{{\\bf z}^F} {\\bf a}^F = \n",
    "\\begin{bmatrix} \n",
    "    \\dfrac{\\partial a^F_1}{\\partial z^F_1} & \\dfrac{\\partial a^F_1}{\\partial z^F_2} & \\cdots & \\dfrac{\\partial a^F_1}{\\partial z^F_k} \\\\ ~ \\\\\n",
    "    \\dfrac{\\partial a^F_2}{\\partial z^F_1} & \\dfrac{\\partial a^F_2}{\\partial z^F_2} & \\cdots & \\dfrac{\\partial a^F_2}{\\partial z^F_k} \\\\ ~ \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\ ~\\\\\n",
    "    \\dfrac{\\partial a^F_k}{\\partial z^F_1} & \\dfrac{\\partial a^F_k}{\\partial z^F_2} & \\cdots & \\dfrac{\\partial a^F_k}{\\partial z^F_k}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Using your answer from part 3B, define a function for computing the matrix $\\nabla_{{\\bf z}^F} {\\bf a}^F$. \n",
    "\n",
    "This seems crazy, but your answer should be fairly nice if you write it in terms of softmax. You also could do this in a for loop if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_derivative(x):\n",
    "    \"\"\"\n",
    "        Add your code below\n",
    "    \"\"\"\n",
    "    s = softmax(x)\n",
    "    deriv =\n",
    "    return deriv\n",
    "\n",
    "# test your function with some example values\n",
    "print(softmax_derivative(x)) # should be array([[ 0.22451502, -0.05907744, -0.16543758], [-0.05907744,  0.17540056, -0.11632312], [-0.16543758, -0.11632312,  0.2817607 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2D**\n",
    "\n",
    "Where does the activation function of the last layer appear in our `backpropagation` function? Will the derivative of softmax change how we compute the error vector ${\\bf d}^F$?\n",
    "\n",
    "Discuss in a senetence or two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 3**\n",
    "\n",
    "At this point, we could work on adapting your `backpropagation` function to use softmax in the final layer, but for the sake of time let's instead rely on TensorFlow to do the hard work for us.\n",
    "\n",
    "Using TensorFlow, design and train a FNN on the MNIST dataset using `tf.activations.softmax` in your output layer, `tf.activations.relu` for your hidden layers, `tf.losses.CategoricalCrossentropy` for your loss function, and `tf.optimizers.SGD(learning_rate=0.01)`.\n",
    "\n",
    "*Tip: It may be helpful to review your submission for Project 1!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 4A**\n",
    "\n",
    "During our Project 2 presentations, we saw that not only are there different kinds of activation and loss functions but there are also different kinds of optimizers!\n",
    "\n",
    "Experiment with using the optimizer `tf.optimizers.Adam(learning_rate=0.01)`. \n",
    "\n",
    "The Adam optimizer is just a variation of gradient descent. Rather than simply updating by a multiple of the gradient of our weights and biases, we include other terms in the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 4B**\n",
    "\n",
    "What did you find? Explain how Adam affected your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Stop Training?\n",
    "\n",
    "Deciding when to stop training is nontrivial. So far, we've relied on our model's performance on training data. However, in practice our model will be used to make decisions on data its never seen before. So, it's important to evaluate its performance on data its not trained on. This leads us to partition our data into training, validation, and testing sets--which we saw in the Project 2 presentations!\n",
    "\n",
    "We'll use `x_test` and `y_test` as our validation dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 5A**\n",
    "\n",
    "Add the `x_test` and `y_test` as a validation dataset to your `mode.fit` command. You can do this by adding `validation_data = (x_test, y_test)` to your inputs and retrain your model to demonstrate how this changes the output of `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 5B**\n",
    "\n",
    "Discuss how we should use the output of our validation data to determine when to stop training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 6**\n",
    "\n",
    "Revisit the Fashion MNIST dataset and train a new model with your new found knowledge.\n",
    "\n",
    "Include your code in the code cell below and use the final markdown cell below that to explain why you've trained a reasonable model. Be sure to reference your validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
